<h1 align="center">ðŸš€ MiniGPT â€” Train Your Own GPT Model from Scratch</h1>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10-blue?logo=python" />
  <img src="https://img.shields.io/badge/PyTorch-2.5.1-red?logo=pytorch" />
  <img src="https://img.shields.io/badge/CUDA-12.9-green?logo=nvidia" />
  <img src="https://img.shields.io/badge/VS_Code-Project-blue?logo=visualstudiocode" />
  <img src="https://img.shields.io/badge/Model-GPT%20from%20Scratch-orange?logo=openai" />
  <img src="https://img.shields.io/badge/Device-GTX%201650-lightgrey?logo=nvidia" />
  <img src="https://img.shields.io/badge/Training-AMP%20Enabled-yellow?logo=lightning" />
  <img src="https://img.shields.io/badge/Status-Active-success?logo=github" />
</p>

---

## ðŸ“Œ Project Overview

MiniGPT is a **fully-custom, from-scratch GPT-style language model** implemented using **PyTorch**, trained locally on an **NVIDIA GTX 1650** GPU.  
This project demonstrates a deep understanding of:

- Transformer architectures  
- Tokenization (SentencePiece)  
- Multi-head self-attention  
- Positional embeddings  
- Training pipelines  
- Mixed-precision (AMP)  
- Checkpointing  
- Inference serving  

Everything is coded manuallyâ€”**no HuggingFace model classes**.

---

## ðŸ“‚ Repository Structure
llm-local/
â”‚
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ small.json # Model config
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw .txt files for tokenizer + training
â”‚ â”œâ”€â”€ processed/ # Preprocessed binary/token files
â”‚ â””â”€â”€ tokenizer/ # SentencePiece tokenizer outputs
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ preprocess.bat # Tokenizer training script
â”‚ â””â”€â”€ run_train.bat # Training runner
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ model/
â”‚ â”‚ â””â”€â”€ transformer.py # Full GPT model from scratch
â”‚ â”œâ”€â”€ server/
â”‚ â”‚ â””â”€â”€ server.py # FastAPI inference server
â”‚ â”œâ”€â”€ data_pipeline.py # Dataset + dataloader
â”‚ â”œâ”€â”€ tokenizer.py # SentencePiece tokenizer builder
â”‚ â””â”€â”€ train.py # Training loop + AMP + checkpoints
â”‚
â””â”€â”€ requirements.txt


---

## ðŸ§  Model Architecture

- **Embedding Layer**  
- **Positional Encoding**  
- **N Transformer Blocks**
  - Multi-head attention  
  - Feed-forward MLP  
  - LayerNorm  
  - Residual connections  
- **Language Modeling Head (LM Head)**  

Configurable via `configs/small.json`:

```json
{
    "vocab_size": 20000,
    "seq_len": 128,
    "n_layer": 6,
    "n_head": 6,
    "d_model": 384,
    "ff_dim": 1536
}
